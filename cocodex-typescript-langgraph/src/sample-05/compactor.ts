/**
 * Context Compactor ì‹œìŠ¤í…œ
 *
 * AIë¥¼ í™œìš©í•œ ë©”ì‹œì§€ ìš”ì•½ ê¸°ëŠ¥
 *
 * í•µì‹¬ ê¸°ëŠ¥:
 * - í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° (tiktoken ì‚¬ìš©)
 * - context windowì˜ n% ì´ìƒ ì‚¬ìš© ì‹œ AIë¡œ ë©”ì‹œì§€ ìš”ì•½
 * - ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ìµœê·¼ ë©”ì‹œì§€ëŠ” ë³´ì¡´
 * - ì¤‘ê°„ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½í•˜ì—¬ ì••ì¶•
 */

import type { BaseMessage } from "@langchain/core/messages";
import {
	AIMessage,
	HumanMessage,
	SystemMessage,
} from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import { encoding_for_model, type TiktokenModel } from "tiktoken";
import type { SessionManager } from "../sample-02/session.js";
import type { CommandResult } from "../sample-03/commands.js";

// ========== CompactionOptions ==========

export interface CompactionOptions {
	/** Context window í¬ê¸° (í† í° ìˆ˜) */
	contextWindowSize: number;
	/** Compactionì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ì„ê³„ê°’ (0.0 ~ 1.0) */
	threshold: number;
	/** ë³´ì¡´í•  ìµœê·¼ ë©”ì‹œì§€ ê°œìˆ˜ */
	preserveRecentCount: number;
}

export interface CompactionResult {
	/** ì••ì¶•ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ */
	compacted: boolean;
	/** ì••ì¶• ì „ ë©”ì‹œì§€ ê°œìˆ˜ */
	originalCount: number;
	/** ì••ì¶• í›„ ë©”ì‹œì§€ ê°œìˆ˜ */
	compactedCount: number;
	/** ì••ì¶• ì „ í† í° ìˆ˜ (ì¶”ì •) */
	originalTokens: number;
	/** ì••ì¶• í›„ í† í° ìˆ˜ (ì¶”ì •) */
	compactedTokens: number;
	/** ì••ì¶•ëœ ë©”ì‹œì§€ ë°°ì—´ */
	messages: BaseMessage[];
}

// ========== ContextCompactor ==========

export class ContextCompactor {
	private options: Required<CompactionOptions>;
	private encoding: ReturnType<typeof encoding_for_model>;

	constructor(options: Partial<CompactionOptions> = {}) {
		this.options = {
			contextWindowSize: options.contextWindowSize || 128000,
			threshold: options.threshold || 0.7,
			preserveRecentCount: options.preserveRecentCount || 4,
		};

		// tiktoken ì´ˆê¸°í™”
		const modelName = process.env.OPENAI_MODEL || "gpt-5";
		this.encoding = encoding_for_model(modelName as TiktokenModel);
	}

	/**
	 * ë©”ì‹œì§€ ë°°ì—´ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚° (tiktoken ì‚¬ìš©)
	 *
	 * ì´ë¯¸ì§€ì˜ ê²½ìš° OpenAI Vision API í† í° ê³„ì‚° ê³µì‹ ì‚¬ìš©:
	 * - ì €í•´ìƒë„(detail=low): 85 í† í°
	 * - ê³ í•´ìƒë„(detail=high): (width/512) * (height/512) * 170 + 85 í† í°
	 * - detail ì§€ì • ì•ˆ ë¨: ê¸°ë³¸ì ìœ¼ë¡œ ê³ í•´ìƒë„ë¡œ ê°„ì£¼í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì¶”ì •
	 */
	estimateTokens(messages: BaseMessage[]): number {
		let totalTokens = 0;

		for (const msg of messages) {
			// ì´ë¯¸ì§€ ë©”ì‹œì§€ ì²˜ë¦¬
			if (typeof msg.content !== "string" && Array.isArray(msg.content)) {
				for (const item of msg.content) {
					// biome-ignore lint/suspicious/noExplicitAny: content item type varies
					const contentItem = item as any;
					if (contentItem.type === "image_url") {
						// OpenAI Vision API í† í° ê³„ì‚°
						// detailì´ lowë©´ 85, ì•„ë‹ˆë©´ í‰ê· ì ìœ¼ë¡œ 255-765 í† í°
						// ì •í™•í•œ ê³„ì‚°ì€ ì´ë¯¸ì§€ í¬ê¸°ê°€ í•„ìš”í•˜ë¯€ë¡œ ì¤‘ê°„ê°’ ì‚¬ìš©
						const detail = contentItem.image_url?.detail || "auto";
						if (detail === "low") {
							totalTokens += 85;
						} else {
							// high ë˜ëŠ” auto: í‰ê· ì ìœ¼ë¡œ 512x512 ì´ë¯¸ì§€ ê¸°ì¤€
							// (512/512) * (512/512) * 170 + 85 = 255 í† í°
							// ë³´ìˆ˜ì ìœ¼ë¡œ ì•½ê°„ ë†’ê²Œ ì¡ì•„ 400 í† í°ìœ¼ë¡œ ì¶”ì •
							totalTokens += 400;
						}
					} else if (contentItem.type === "text") {
						totalTokens += this.encoding.encode(contentItem.text || "").length;
					}
				}
			} else {
				// ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
				const content =
					typeof msg.content === "string"
						? msg.content
						: JSON.stringify(msg.content);

				totalTokens += this.encoding.encode(content).length;
			}

			totalTokens += 4; // ë©”íƒ€ë°ì´í„°
		}

		totalTokens += 3; // êµ¬ì¡° ì˜¤ë²„í—¤ë“œ
		return totalTokens;
	}

	/**
	 * Context window ì‚¬ìš©ë¥  ê³„ì‚°
	 */
	private calculateUsageRatio(messages: BaseMessage[]): number {
		const tokens = this.estimateTokens(messages);
		return tokens / this.options.contextWindowSize;
	}

	/**
	 * ë©”ì‹œì§€ ë°°ì—´ì„ ì••ì¶•í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
	 */
	shouldCompact(messages: BaseMessage[]): boolean {
		const usageRatio = this.calculateUsageRatio(messages);
		return usageRatio >= this.options.threshold;
	}

	/**
	 * ë©”ì‹œì§€ ë°°ì—´ì„ AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì••ì¶•
	 *
	 * @param messages - ì••ì¶•í•  ë©”ì‹œì§€ ë°°ì—´
	 * @param force - trueì´ë©´ ì„ê³„ê°’ ì²´í¬ ì—†ì´ ë¬´ì¡°ê±´ ì••ì¶• (ê¸°ë³¸ê°’: false)
	 */
	async compactMessages(
		messages: BaseMessage[],
		force: boolean = false,
	): Promise<CompactionResult> {
		const originalTokens = this.estimateTokens(messages);
		const usageRatio = this.calculateUsageRatio(messages);

		console.log(
			`\nğŸ“Š Context Window ì‚¬ìš©ë¥ : ${(usageRatio * 100).toFixed(1)}%`,
		);
		console.log(
			`   ì¶”ì • í† í°: ${originalTokens} / ${this.options.contextWindowSize}`,
		);

		// ì••ì¶•ì´ í•„ìš” ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (force=trueì´ë©´ ìŠ¤í‚µ)
		if (!force && !this.shouldCompact(messages)) {
			console.log("   âœ… ì••ì¶• ë¶ˆí•„ìš”");
			return {
				compacted: false,
				originalCount: messages.length,
				compactedCount: messages.length,
				originalTokens,
				compactedTokens: originalTokens,
				messages,
			};
		}

		console.log(force ? "   ğŸ”„ ê°•ì œ ì••ì¶• ì‹œì‘..." : "   ğŸ”„ ì••ì¶• ì‹œì‘...");

		// 1. ë©”ì‹œì§€ ë¶„ë¥˜
		const systemMessages: BaseMessage[] = [];
		const middleMessages: BaseMessage[] = [];
		const recentMessages: BaseMessage[] = [];

		// ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¶„ë¦¬
		let nonSystemMessages = messages;
		for (let i = 0; i < messages.length; i++) {
			if (messages[i]._getType() === "system") {
				systemMessages.push(messages[i]);
			} else {
				nonSystemMessages = messages.slice(i);
				break;
			}
		}

		// ìµœê·¼ ë©”ì‹œì§€ ë¶„ë¦¬
		if (nonSystemMessages.length > this.options.preserveRecentCount) {
			const splitIndex =
				nonSystemMessages.length - this.options.preserveRecentCount;
			middleMessages.push(...nonSystemMessages.slice(0, splitIndex));
			recentMessages.push(...nonSystemMessages.slice(splitIndex));
		} else {
			recentMessages.push(...nonSystemMessages);
		}

		// 2. ì¤‘ê°„ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì••ì¶•í•  ê²ƒì´ ì—†ìŒ
		if (middleMessages.length === 0) {
			console.log("   âš ï¸  ì••ì¶•í•  ì¤‘ê°„ ë©”ì‹œì§€ê°€ ì—†ìŒ");
			return {
				compacted: false,
				originalCount: messages.length,
				compactedCount: messages.length,
				originalTokens,
				compactedTokens: originalTokens,
				messages,
			};
		}

		// 3. ì¤‘ê°„ ë©”ì‹œì§€ë¥¼ AIë¡œ ìš”ì•½
		console.log(`   ğŸ“ ì¤‘ê°„ ë©”ì‹œì§€ ìš”ì•½ ì¤‘... (${middleMessages.length}ê°œ)`);
		const summary = await this.summarizeMessages(middleMessages);

		// 4. ì••ì¶•ëœ ë©”ì‹œì§€ ë°°ì—´ ìƒì„±
		const compactedMessages: BaseMessage[] = [
			...systemMessages,
			new SystemMessage(`[ì´ì „ ëŒ€í™” ìš”ì•½]\n${summary}`),
			...recentMessages,
		];

		const compactedTokens = this.estimateTokens(compactedMessages);

		console.log(`   âœ… ì••ì¶• ì™„ë£Œ!`);
		console.log(
			`      ë©”ì‹œì§€: ${messages.length}ê°œ â†’ ${compactedMessages.length}ê°œ`,
		);
		console.log(
			`      í† í°: ${originalTokens} â†’ ${compactedTokens} (${((1 - compactedTokens / originalTokens) * 100).toFixed(1)}% ê°ì†Œ)`,
		);

		return {
			compacted: true,
			originalCount: messages.length,
			compactedCount: compactedMessages.length,
			originalTokens,
			compactedTokens,
			messages: compactedMessages,
		};
	}

	/**
	 * ë©”ì‹œì§€ ë°°ì—´ì„ AIë¡œ ìš”ì•½
	 */
	private async summarizeMessages(messages: BaseMessage[]): Promise<string> {
		const model = new ChatOpenAI({
			modelName: process.env.OPENAI_MODEL || "gpt-5",
			temperature: parseFloat(process.env.OPENAI_TEMPERATURE || "1"),
		});

		// ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
		let conversation = "";
		for (const msg of messages) {
			const type = msg._getType();
			const content =
				typeof msg.content === "string"
					? msg.content
					: JSON.stringify(msg.content);

			let role = "Unknown";
			if (type === "human") role = "User";
			else if (type === "ai") role = "Assistant";
			else if (type === "system") role = "System";
			else if (type === "tool") role = "Tool";

			conversation += `${role}: ${content}\n\n`;
		}

		// ìš”ì•½ í”„ë¡¬í”„íŠ¸
		const summaryPrompt = `ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤. ì´ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì—ëŠ” ë‹¤ìŒ ì •ë³´ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
- ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì£¼ìš” ì‘ì—…
- ìˆ˜í–‰ëœ ì‘ì—…ê³¼ ê²°ê³¼
- ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë‚˜ ê²°ì •ì‚¬í•­

ìš”ì•½ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ìƒëµí•˜ì„¸ìš”.

ëŒ€í™” ê¸°ë¡:
${conversation}

ìš”ì•½:`;

		const response = await model.invoke([new HumanMessage(summaryPrompt)]);

		return typeof response.content === "string"
			? response.content
			: JSON.stringify(response.content);
	}

	/**
	 * ì „ì²´ ë©”ì‹œì§€ë¥¼ AIë¡œ ìš”ì•½ (ê°„ë‹¨ ë²„ì „)
	 * /compact ëª…ë ¹ì–´ìš©
	 */
	private async summarizeAllMessages(messages: BaseMessage[]): Promise<string> {
		const model = new ChatOpenAI({
			modelName: process.env.OPENAI_MODEL || "gpt-5",
			temperature: parseFloat(process.env.OPENAI_TEMPERATURE || "1"),
		});

		// ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
		let conversation = "";
		for (const msg of messages) {
			const type = msg._getType();
			if (type === "system") continue; // ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìŠ¤í‚µ

			const content =
				typeof msg.content === "string"
					? msg.content
					: JSON.stringify(msg.content);

			let role = "Unknown";
			if (type === "human") role = "User";
			else if (type === "ai") role = "Assistant";
			else if (type === "tool") role = "Tool";

			conversation += `${role}: ${content}\n\n`;
		}

		// ìš”ì•½ í”„ë¡¬í”„íŠ¸
		const summaryPrompt = `ë‹¤ìŒ ëŒ€í™” ë‚´ì—­ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

${conversation}

ìš”ì•½:`;

		const response = await model.invoke([new HumanMessage(summaryPrompt)]);

		return typeof response.content === "string"
			? response.content
			: JSON.stringify(response.content);
	}

	/**
	 * í˜„ì¬ ì„¤ì • ì •ë³´ ë°˜í™˜
	 */
	getOptions(): Required<CompactionOptions> {
		return { ...this.options };
	}

	/**
	 * ì„¤ì • ì—…ë°ì´íŠ¸
	 */
	updateOptions(options: Partial<CompactionOptions>): void {
		this.options = {
			...this.options,
			...options,
		};
	}

	// ========== CommandRegistry ìœ í‹¸ í•¨ìˆ˜ ==========

	/**
	 * /compact ëª…ë ¹ì–´ í•¸ë“¤ëŸ¬ ìƒì„±
	 *
	 * commandRegistry.register("compact", compactor.handlerCompact(sessionManager))
	 * í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
	 *
	 * @param sessionManager - ì„¸ì…˜ ê´€ë¦¬ì
	 */
	handlerCompact(sessionManager: SessionManager) {
		// biome-ignore lint/suspicious/noExplicitAny: Context is dynamic
		return async (_args: string, context?: any): Promise<CommandResult> => {
			const sessionId =
				context?.sessionId || sessionManager.getCurrentSessionId();

			console.log("\nğŸ—œï¸  ì••ì¶• ì‹¤í–‰...");

			const messages = sessionManager.getMessages(sessionId);
			const originalCount = messages.length;

			// 1. AIì—ê²Œ ì „ì²´ ëŒ€í™” ìš”ì•½ ìš”ì²­
			console.log("   ğŸ“ AIì—ê²Œ ëŒ€í™” ìš”ì•½ ìš”ì²­ ì¤‘...");
			const summary = await this.summarizeAllMessages(messages);

			// 2. ìƒˆ ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±: SYSTEM + Context + ìš”ì•½
			const systemMessages = messages.filter((m) => m._getType() === "system");
			const contextMessages = messages.filter(
				(m) =>
					m._getType() === "human" &&
					typeof m.content === "string" &&
					m.content.includes("cocoagent.md"),
			);

			const newMessages: BaseMessage[] = [
				...systemMessages,
				...contextMessages,
				new AIMessage(`[ì´ì „ ëŒ€í™” ìš”ì•½]\n${summary}`),
			];

			// 3. ì„¸ì…˜ ë®ì–´ì“°ê¸°
			sessionManager.replaceMessages(newMessages, sessionId);

			const newCount = newMessages.length;
			const originalTokens = this.estimateTokens(messages);
			const newTokens = this.estimateTokens(newMessages);

			console.log("âœ… ì••ì¶• ì™„ë£Œ");
			console.log(`   ë©”ì‹œì§€: ${originalCount}ê°œ â†’ ${newCount}ê°œ`);
			console.log(`   í† í°: ${originalTokens} â†’ ${newTokens}`);
			console.log(
				`   ê°ì†Œìœ¨: ${((1 - newTokens / originalTokens) * 100).toFixed(1)}%`,
			);

			return { type: "executed" }; // ì‹¤í–‰ ì™„ë£Œ, ë‹¤ì‹œ ì…ë ¥ ëŒ€ê¸°
		};
	}

	/**
	 * /status ëª…ë ¹ì–´ í•¸ë“¤ëŸ¬ ìƒì„±
	 *
	 * commandRegistry.register("status", compactor.handlerStatus(sessionManager))
	 * í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
	 *
	 * @param sessionManager - ì„¸ì…˜ ê´€ë¦¬ì
	 */
	handlerStatus(sessionManager: SessionManager) {
		// biome-ignore lint/suspicious/noExplicitAny: Context is dynamic
		return async (_args: string, context?: any): Promise<CommandResult> => {
			const sessionId =
				context?.sessionId || sessionManager.getCurrentSessionId();

			const messages = sessionManager.getMessages(sessionId);
			const tokens = this.estimateTokens(messages);
			const ratio = (tokens / this.options.contextWindowSize) * 100;

			console.log("\nğŸ“Š ì„¸ì…˜ ìƒíƒœ:");
			console.log(`   ë©”ì‹œì§€: ${messages.length}ê°œ`);
			console.log(`   í† í°: ${tokens} / ${this.options.contextWindowSize}`);
			console.log(`   ì‚¬ìš©ë¥ : ${ratio.toFixed(1)}%`);
			console.log(
				`   ì••ì¶• ì„ê³„ê°’: ${(this.options.threshold * 100).toFixed(0)}%`,
			);

			if (ratio >= this.options.threshold * 100) {
				console.log("   âš ï¸  ì••ì¶• ê¶Œì¥");
			} else {
				console.log("   âœ… ì •ìƒ");
			}

			return { type: "executed" }; // ì‹¤í–‰ ì™„ë£Œ, ë‹¤ì‹œ ì…ë ¥ ëŒ€ê¸°
		};
	}
}
